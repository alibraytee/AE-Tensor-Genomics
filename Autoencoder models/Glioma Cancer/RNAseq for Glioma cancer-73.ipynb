{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d9e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable as V\n",
    "import torch.nn.functional as F\n",
    "from  sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96b76a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in datasets\n",
    "RNAseq=pd.read_csv(\"/Users/vivit/CC/data_rnaSeq_G.csv\")\n",
    "train_clinical=pd.read_csv(\"/Users/vivit/CC/train_clinical_G.csv\")\n",
    "test_clinical=pd.read_csv(\"/Users/vivit/CC/test_clinical_G.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e571110",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx=train_clinical[['Unnamed: 0.1']]\n",
    "test_idx=test_clinical[['Unnamed: 0.1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5826c0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2BP1</th>\n",
       "      <th>A2LD1</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>A4GNT</th>\n",
       "      <th>AAA1</th>\n",
       "      <th>...</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "      <th>psiTPTE22</th>\n",
       "      <th>tAKR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA.CS.4938</td>\n",
       "      <td>6.5715</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.5641</td>\n",
       "      <td>3.8564</td>\n",
       "      <td>7.1997</td>\n",
       "      <td>13.8518</td>\n",
       "      <td>6.2176</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0266</td>\n",
       "      <td>9.0339</td>\n",
       "      <td>10.3581</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.5476</td>\n",
       "      <td>10.6438</td>\n",
       "      <td>10.1957</td>\n",
       "      <td>9.7094</td>\n",
       "      <td>9.6052</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA.CS.4941</td>\n",
       "      <td>6.1944</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0375</td>\n",
       "      <td>7.1808</td>\n",
       "      <td>9.0290</td>\n",
       "      <td>14.1314</td>\n",
       "      <td>7.3288</td>\n",
       "      <td>1.0252</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.7406</td>\n",
       "      <td>8.8721</td>\n",
       "      <td>10.2618</td>\n",
       "      <td>0.4277</td>\n",
       "      <td>10.7750</td>\n",
       "      <td>11.9259</td>\n",
       "      <td>10.7387</td>\n",
       "      <td>9.9894</td>\n",
       "      <td>4.3204</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA.CS.4942</td>\n",
       "      <td>6.2375</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.5295</td>\n",
       "      <td>5.7117</td>\n",
       "      <td>7.4545</td>\n",
       "      <td>14.2341</td>\n",
       "      <td>5.5011</td>\n",
       "      <td>0.4287</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.8608</td>\n",
       "      <td>8.8308</td>\n",
       "      <td>10.1078</td>\n",
       "      <td>0.4287</td>\n",
       "      <td>10.6322</td>\n",
       "      <td>11.1134</td>\n",
       "      <td>10.4766</td>\n",
       "      <td>9.4674</td>\n",
       "      <td>10.7374</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA.CS.4943</td>\n",
       "      <td>4.9535</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.5235</td>\n",
       "      <td>3.9052</td>\n",
       "      <td>7.4920</td>\n",
       "      <td>13.5168</td>\n",
       "      <td>5.1915</td>\n",
       "      <td>1.2043</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.4921</td>\n",
       "      <td>8.9427</td>\n",
       "      <td>10.3324</td>\n",
       "      <td>2.0911</td>\n",
       "      <td>10.7823</td>\n",
       "      <td>11.0584</td>\n",
       "      <td>10.9054</td>\n",
       "      <td>9.4311</td>\n",
       "      <td>8.0716</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA.CS.4944</td>\n",
       "      <td>4.6844</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.7335</td>\n",
       "      <td>4.2491</td>\n",
       "      <td>7.3253</td>\n",
       "      <td>13.4115</td>\n",
       "      <td>6.8579</td>\n",
       "      <td>0.5892</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.4385</td>\n",
       "      <td>8.3434</td>\n",
       "      <td>9.9855</td>\n",
       "      <td>0.5892</td>\n",
       "      <td>10.2598</td>\n",
       "      <td>10.7259</td>\n",
       "      <td>9.5133</td>\n",
       "      <td>8.8390</td>\n",
       "      <td>4.3694</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>TCGA.WY.A85A</td>\n",
       "      <td>6.3097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.4914</td>\n",
       "      <td>4.8743</td>\n",
       "      <td>8.3211</td>\n",
       "      <td>13.1848</td>\n",
       "      <td>5.7910</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.7772</td>\n",
       "      <td>8.4595</td>\n",
       "      <td>10.3039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.5437</td>\n",
       "      <td>10.5493</td>\n",
       "      <td>10.6046</td>\n",
       "      <td>9.3784</td>\n",
       "      <td>3.0686</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>TCGA.WY.A85B</td>\n",
       "      <td>6.4383</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.4604</td>\n",
       "      <td>4.9876</td>\n",
       "      <td>5.3276</td>\n",
       "      <td>14.1384</td>\n",
       "      <td>5.5559</td>\n",
       "      <td>1.4442</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9364</td>\n",
       "      <td>8.9773</td>\n",
       "      <td>10.3973</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>10.4965</td>\n",
       "      <td>10.6219</td>\n",
       "      <td>10.7201</td>\n",
       "      <td>9.7107</td>\n",
       "      <td>12.8674</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>TCGA.WY.A85C</td>\n",
       "      <td>6.9881</td>\n",
       "      <td>0.4944</td>\n",
       "      <td>5.5221</td>\n",
       "      <td>4.5847</td>\n",
       "      <td>5.1757</td>\n",
       "      <td>13.4168</td>\n",
       "      <td>6.5510</td>\n",
       "      <td>1.1545</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2746</td>\n",
       "      <td>9.0549</td>\n",
       "      <td>10.6047</td>\n",
       "      <td>1.9488</td>\n",
       "      <td>10.8586</td>\n",
       "      <td>10.9134</td>\n",
       "      <td>10.9709</td>\n",
       "      <td>9.8315</td>\n",
       "      <td>11.5579</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>TCGA.WY.A85D</td>\n",
       "      <td>8.2396</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.7356</td>\n",
       "      <td>3.9758</td>\n",
       "      <td>8.0754</td>\n",
       "      <td>13.5098</td>\n",
       "      <td>6.0992</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.1703</td>\n",
       "      <td>8.2906</td>\n",
       "      <td>10.5089</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.2572</td>\n",
       "      <td>10.7373</td>\n",
       "      <td>10.4823</td>\n",
       "      <td>9.5928</td>\n",
       "      <td>10.6111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>TCGA.WY.A85E</td>\n",
       "      <td>7.0297</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.3438</td>\n",
       "      <td>3.7495</td>\n",
       "      <td>7.4474</td>\n",
       "      <td>12.4701</td>\n",
       "      <td>5.0503</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4627</td>\n",
       "      <td>...</td>\n",
       "      <td>5.8221</td>\n",
       "      <td>8.5892</td>\n",
       "      <td>9.5094</td>\n",
       "      <td>2.1384</td>\n",
       "      <td>10.6923</td>\n",
       "      <td>10.9978</td>\n",
       "      <td>10.4872</td>\n",
       "      <td>9.2934</td>\n",
       "      <td>10.2409</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>508 rows × 20119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    A1BG    A1CF   A2BP1   A2LD1   A2ML1      A2M  A4GALT  \\\n",
       "0    TCGA.CS.4938  6.5715  0.0000  4.5641  3.8564  7.1997  13.8518  6.2176   \n",
       "1    TCGA.CS.4941  6.1944  0.0000  9.0375  7.1808  9.0290  14.1314  7.3288   \n",
       "2    TCGA.CS.4942  6.2375  0.0000  8.5295  5.7117  7.4545  14.2341  5.5011   \n",
       "3    TCGA.CS.4943  4.9535  0.0000  5.5235  3.9052  7.4920  13.5168  5.1915   \n",
       "4    TCGA.CS.4944  4.6844  0.0000  6.7335  4.2491  7.3253  13.4115  6.8579   \n",
       "..            ...     ...     ...     ...     ...     ...      ...     ...   \n",
       "503  TCGA.WY.A85A  6.3097  0.0000  7.4914  4.8743  8.3211  13.1848  5.7910   \n",
       "504  TCGA.WY.A85B  6.4383  0.0000  5.4604  4.9876  5.3276  14.1384  5.5559   \n",
       "505  TCGA.WY.A85C  6.9881  0.4944  5.5221  4.5847  5.1757  13.4168  6.5510   \n",
       "506  TCGA.WY.A85D  8.2396  0.0000  4.7356  3.9758  8.0754  13.5098  6.0992   \n",
       "507  TCGA.WY.A85E  7.0297  0.0000  9.3438  3.7495  7.4474  12.4701  5.0503   \n",
       "\n",
       "      A4GNT    AAA1  ...    ZXDA    ZXDB     ZXDC  ZYG11A   ZYG11B      ZYX  \\\n",
       "0    0.0000  0.0000  ...  6.0266  9.0339  10.3581  0.0000  10.5476  10.6438   \n",
       "1    1.0252  0.0000  ...  6.7406  8.8721  10.2618  0.4277  10.7750  11.9259   \n",
       "2    0.4287  0.0000  ...  6.8608  8.8308  10.1078  0.4287  10.6322  11.1134   \n",
       "3    1.2043  0.0000  ...  6.4921  8.9427  10.3324  2.0911  10.7823  11.0584   \n",
       "4    0.5892  0.0000  ...  5.4385  8.3434   9.9855  0.5892  10.2598  10.7259   \n",
       "..      ...     ...  ...     ...     ...      ...     ...      ...      ...   \n",
       "503  0.6115  0.0000  ...  5.7772  8.4595  10.3039  0.0000  10.5437  10.5493   \n",
       "504  1.4442  0.0000  ...  5.9364  8.9773  10.3973  0.8958  10.4965  10.6219   \n",
       "505  1.1545  0.0000  ...  6.2746  9.0549  10.6047  1.9488  10.8586  10.9134   \n",
       "506  0.0000  0.0000  ...  5.1703  8.2906  10.5089  0.0000  10.2572  10.7373   \n",
       "507  0.0000  0.4627  ...  5.8221  8.5892   9.5094  2.1384  10.6923  10.9978   \n",
       "\n",
       "       ZZEF1    ZZZ3  psiTPTE22  tAKR  \n",
       "0    10.1957  9.7094     9.6052   0.0  \n",
       "1    10.7387  9.9894     4.3204   0.0  \n",
       "2    10.4766  9.4674    10.7374   0.0  \n",
       "3    10.9054  9.4311     8.0716   0.0  \n",
       "4     9.5133  8.8390     4.3694   0.0  \n",
       "..       ...     ...        ...   ...  \n",
       "503  10.6046  9.3784     3.0686   0.0  \n",
       "504  10.7201  9.7107    12.8674   0.0  \n",
       "505  10.9709  9.8315    11.5579   0.0  \n",
       "506  10.4823  9.5928    10.6111   0.0  \n",
       "507  10.4872  9.2934    10.2409   0.0  \n",
       "\n",
       "[508 rows x 20119 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNAseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e22b970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(508, 20119)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNAseq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7581d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = RNAseq.merge(train_idx,left_on='Unnamed: 0', right_on='Unnamed: 0.1',how=\"inner\")\n",
    "testset = RNAseq.merge(test_idx,left_on='Unnamed: 0', right_on='Unnamed: 0.1',how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85fd7966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2BP1</th>\n",
       "      <th>A2LD1</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>A4GNT</th>\n",
       "      <th>AAA1</th>\n",
       "      <th>...</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "      <th>psiTPTE22</th>\n",
       "      <th>tAKR</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA.CS.4941</td>\n",
       "      <td>6.1944</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0375</td>\n",
       "      <td>7.1808</td>\n",
       "      <td>9.0290</td>\n",
       "      <td>14.1314</td>\n",
       "      <td>7.3288</td>\n",
       "      <td>1.0252</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.8721</td>\n",
       "      <td>10.2618</td>\n",
       "      <td>0.4277</td>\n",
       "      <td>10.7750</td>\n",
       "      <td>11.9259</td>\n",
       "      <td>10.7387</td>\n",
       "      <td>9.9894</td>\n",
       "      <td>4.3204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TCGA.CS.4941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA.CS.4942</td>\n",
       "      <td>6.2375</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.5295</td>\n",
       "      <td>5.7117</td>\n",
       "      <td>7.4545</td>\n",
       "      <td>14.2341</td>\n",
       "      <td>5.5011</td>\n",
       "      <td>0.4287</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.8308</td>\n",
       "      <td>10.1078</td>\n",
       "      <td>0.4287</td>\n",
       "      <td>10.6322</td>\n",
       "      <td>11.1134</td>\n",
       "      <td>10.4766</td>\n",
       "      <td>9.4674</td>\n",
       "      <td>10.7374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TCGA.CS.4942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA.CS.4943</td>\n",
       "      <td>4.9535</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.5235</td>\n",
       "      <td>3.9052</td>\n",
       "      <td>7.4920</td>\n",
       "      <td>13.5168</td>\n",
       "      <td>5.1915</td>\n",
       "      <td>1.2043</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.9427</td>\n",
       "      <td>10.3324</td>\n",
       "      <td>2.0911</td>\n",
       "      <td>10.7823</td>\n",
       "      <td>11.0584</td>\n",
       "      <td>10.9054</td>\n",
       "      <td>9.4311</td>\n",
       "      <td>8.0716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TCGA.CS.4943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA.CS.4944</td>\n",
       "      <td>4.6844</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.7335</td>\n",
       "      <td>4.2491</td>\n",
       "      <td>7.3253</td>\n",
       "      <td>13.4115</td>\n",
       "      <td>6.8579</td>\n",
       "      <td>0.5892</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3434</td>\n",
       "      <td>9.9855</td>\n",
       "      <td>0.5892</td>\n",
       "      <td>10.2598</td>\n",
       "      <td>10.7259</td>\n",
       "      <td>9.5133</td>\n",
       "      <td>8.8390</td>\n",
       "      <td>4.3694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TCGA.CS.4944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA.CS.5393</td>\n",
       "      <td>5.5662</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.3424</td>\n",
       "      <td>4.9114</td>\n",
       "      <td>7.3717</td>\n",
       "      <td>14.0085</td>\n",
       "      <td>6.2716</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.6205</td>\n",
       "      <td>10.3663</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.6882</td>\n",
       "      <td>10.7732</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>9.7250</td>\n",
       "      <td>9.7346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TCGA.CS.5393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>TCGA.WY.A858</td>\n",
       "      <td>7.7583</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.6741</td>\n",
       "      <td>5.3648</td>\n",
       "      <td>5.5743</td>\n",
       "      <td>15.3476</td>\n",
       "      <td>7.9331</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.2489</td>\n",
       "      <td>10.0079</td>\n",
       "      <td>0.6763</td>\n",
       "      <td>9.8439</td>\n",
       "      <td>11.4981</td>\n",
       "      <td>10.3045</td>\n",
       "      <td>8.8761</td>\n",
       "      <td>9.2236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TCGA.WY.A858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>TCGA.WY.A85A</td>\n",
       "      <td>6.3097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.4914</td>\n",
       "      <td>4.8743</td>\n",
       "      <td>8.3211</td>\n",
       "      <td>13.1848</td>\n",
       "      <td>5.7910</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4595</td>\n",
       "      <td>10.3039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.5437</td>\n",
       "      <td>10.5493</td>\n",
       "      <td>10.6046</td>\n",
       "      <td>9.3784</td>\n",
       "      <td>3.0686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TCGA.WY.A85A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>TCGA.WY.A85B</td>\n",
       "      <td>6.4383</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.4604</td>\n",
       "      <td>4.9876</td>\n",
       "      <td>5.3276</td>\n",
       "      <td>14.1384</td>\n",
       "      <td>5.5559</td>\n",
       "      <td>1.4442</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.9773</td>\n",
       "      <td>10.3973</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>10.4965</td>\n",
       "      <td>10.6219</td>\n",
       "      <td>10.7201</td>\n",
       "      <td>9.7107</td>\n",
       "      <td>12.8674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TCGA.WY.A85B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>TCGA.WY.A85C</td>\n",
       "      <td>6.9881</td>\n",
       "      <td>0.4944</td>\n",
       "      <td>5.5221</td>\n",
       "      <td>4.5847</td>\n",
       "      <td>5.1757</td>\n",
       "      <td>13.4168</td>\n",
       "      <td>6.5510</td>\n",
       "      <td>1.1545</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0549</td>\n",
       "      <td>10.6047</td>\n",
       "      <td>1.9488</td>\n",
       "      <td>10.8586</td>\n",
       "      <td>10.9134</td>\n",
       "      <td>10.9709</td>\n",
       "      <td>9.8315</td>\n",
       "      <td>11.5579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TCGA.WY.A85C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>TCGA.WY.A85E</td>\n",
       "      <td>7.0297</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.3438</td>\n",
       "      <td>3.7495</td>\n",
       "      <td>7.4474</td>\n",
       "      <td>12.4701</td>\n",
       "      <td>5.0503</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4627</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5892</td>\n",
       "      <td>9.5094</td>\n",
       "      <td>2.1384</td>\n",
       "      <td>10.6923</td>\n",
       "      <td>10.9978</td>\n",
       "      <td>10.4872</td>\n",
       "      <td>9.2934</td>\n",
       "      <td>10.2409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TCGA.WY.A85E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>355 rows × 20120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    A1BG    A1CF   A2BP1   A2LD1   A2ML1      A2M  A4GALT  \\\n",
       "0    TCGA.CS.4941  6.1944  0.0000  9.0375  7.1808  9.0290  14.1314  7.3288   \n",
       "1    TCGA.CS.4942  6.2375  0.0000  8.5295  5.7117  7.4545  14.2341  5.5011   \n",
       "2    TCGA.CS.4943  4.9535  0.0000  5.5235  3.9052  7.4920  13.5168  5.1915   \n",
       "3    TCGA.CS.4944  4.6844  0.0000  6.7335  4.2491  7.3253  13.4115  6.8579   \n",
       "4    TCGA.CS.5393  5.5662  0.0000  8.3424  4.9114  7.3717  14.0085  6.2716   \n",
       "..            ...     ...     ...     ...     ...     ...      ...     ...   \n",
       "350  TCGA.WY.A858  7.7583  0.0000  2.6741  5.3648  5.5743  15.3476  7.9331   \n",
       "351  TCGA.WY.A85A  6.3097  0.0000  7.4914  4.8743  8.3211  13.1848  5.7910   \n",
       "352  TCGA.WY.A85B  6.4383  0.0000  5.4604  4.9876  5.3276  14.1384  5.5559   \n",
       "353  TCGA.WY.A85C  6.9881  0.4944  5.5221  4.5847  5.1757  13.4168  6.5510   \n",
       "354  TCGA.WY.A85E  7.0297  0.0000  9.3438  3.7495  7.4474  12.4701  5.0503   \n",
       "\n",
       "      A4GNT    AAA1  ...    ZXDB     ZXDC  ZYG11A   ZYG11B      ZYX    ZZEF1  \\\n",
       "0    1.0252  0.0000  ...  8.8721  10.2618  0.4277  10.7750  11.9259  10.7387   \n",
       "1    0.4287  0.0000  ...  8.8308  10.1078  0.4287  10.6322  11.1134  10.4766   \n",
       "2    1.2043  0.0000  ...  8.9427  10.3324  2.0911  10.7823  11.0584  10.9054   \n",
       "3    0.5892  0.0000  ...  8.3434   9.9855  0.5892  10.2598  10.7259   9.5133   \n",
       "4    0.0000  0.0000  ...  8.6205  10.3663  0.0000  10.6882  10.7732  10.9516   \n",
       "..      ...     ...  ...     ...      ...     ...      ...      ...      ...   \n",
       "350  0.0000  0.0000  ...  8.2489  10.0079  0.6763   9.8439  11.4981  10.3045   \n",
       "351  0.6115  0.0000  ...  8.4595  10.3039  0.0000  10.5437  10.5493  10.6046   \n",
       "352  1.4442  0.0000  ...  8.9773  10.3973  0.8958  10.4965  10.6219  10.7201   \n",
       "353  1.1545  0.0000  ...  9.0549  10.6047  1.9488  10.8586  10.9134  10.9709   \n",
       "354  0.0000  0.4627  ...  8.5892   9.5094  2.1384  10.6923  10.9978  10.4872   \n",
       "\n",
       "       ZZZ3  psiTPTE22  tAKR  Unnamed: 0.1  \n",
       "0    9.9894     4.3204   0.0  TCGA.CS.4941  \n",
       "1    9.4674    10.7374   0.0  TCGA.CS.4942  \n",
       "2    9.4311     8.0716   0.0  TCGA.CS.4943  \n",
       "3    8.8390     4.3694   0.0  TCGA.CS.4944  \n",
       "4    9.7250     9.7346   0.0  TCGA.CS.5393  \n",
       "..      ...        ...   ...           ...  \n",
       "350  8.8761     9.2236   0.0  TCGA.WY.A858  \n",
       "351  9.3784     3.0686   0.0  TCGA.WY.A85A  \n",
       "352  9.7107    12.8674   0.0  TCGA.WY.A85B  \n",
       "353  9.8315    11.5579   0.0  TCGA.WY.A85C  \n",
       "354  9.2934    10.2409   0.0  TCGA.WY.A85E  \n",
       "\n",
       "[355 rows x 20120 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bde01194",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_73=trainset[[\"Unnamed: 0\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb1081fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clinical73=train_clinical.merge(train_idx_73,left_on=\"Unnamed: 0.1\",right_on=\"Unnamed: 0\",how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8807d728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>years_to_birth</th>\n",
       "      <th>histological_type</th>\n",
       "      <th>gender</th>\n",
       "      <th>radiation_therapy</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>overall_survival</th>\n",
       "      <th>status</th>\n",
       "      <th>overallsurvival</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474</td>\n",
       "      <td>TCGA.TQ.A7RJ</td>\n",
       "      <td>25.0</td>\n",
       "      <td>oligoastrocytoma</td>\n",
       "      <td>female</td>\n",
       "      <td>yes</td>\n",
       "      <td>white</td>\n",
       "      <td>hispanicorlatino</td>\n",
       "      <td>1229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1229,0</td>\n",
       "      <td>TCGA.TQ.A7RJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>227</td>\n",
       "      <td>TCGA.HT.7468</td>\n",
       "      <td>30.0</td>\n",
       "      <td>oligodendroglioma</td>\n",
       "      <td>male</td>\n",
       "      <td>no</td>\n",
       "      <td>white</td>\n",
       "      <td>nothispanicorlatino</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203,0</td>\n",
       "      <td>TCGA.HT.7468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>TCGA.DB.A75P</td>\n",
       "      <td>25.0</td>\n",
       "      <td>astrocytoma</td>\n",
       "      <td>female</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>492.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>492,0</td>\n",
       "      <td>TCGA.DB.A75P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>320</td>\n",
       "      <td>TCGA.HT.A619</td>\n",
       "      <td>51.0</td>\n",
       "      <td>oligodendroglioma</td>\n",
       "      <td>female</td>\n",
       "      <td>no</td>\n",
       "      <td>asian</td>\n",
       "      <td>nothispanicorlatino</td>\n",
       "      <td>651.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>651,0</td>\n",
       "      <td>TCGA.HT.A619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221</td>\n",
       "      <td>TCGA.FG.A711</td>\n",
       "      <td>33.0</td>\n",
       "      <td>oligodendroglioma</td>\n",
       "      <td>female</td>\n",
       "      <td>no</td>\n",
       "      <td>white</td>\n",
       "      <td>nothispanicorlatino</td>\n",
       "      <td>1481.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1481,1</td>\n",
       "      <td>TCGA.FG.A711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>98</td>\n",
       "      <td>TCGA.DU.7007</td>\n",
       "      <td>33.0</td>\n",
       "      <td>astrocytoma</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>white</td>\n",
       "      <td>nothispanicorlatino</td>\n",
       "      <td>1915.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1915,1</td>\n",
       "      <td>TCGA.DU.7007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>476</td>\n",
       "      <td>TCGA.TQ.A7RM</td>\n",
       "      <td>41.0</td>\n",
       "      <td>oligoastrocytoma</td>\n",
       "      <td>female</td>\n",
       "      <td>yes</td>\n",
       "      <td>white</td>\n",
       "      <td>hispanicorlatino</td>\n",
       "      <td>1116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1116,0</td>\n",
       "      <td>TCGA.TQ.A7RM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>322</td>\n",
       "      <td>TCGA.HT.A61C</td>\n",
       "      <td>66.0</td>\n",
       "      <td>oligodendroglioma</td>\n",
       "      <td>male</td>\n",
       "      <td>yes</td>\n",
       "      <td>white</td>\n",
       "      <td>nothispanicorlatino</td>\n",
       "      <td>537.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>537,1</td>\n",
       "      <td>TCGA.HT.A61C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>382</td>\n",
       "      <td>TCGA.QH.A6CZ</td>\n",
       "      <td>38.0</td>\n",
       "      <td>oligoastrocytoma</td>\n",
       "      <td>male</td>\n",
       "      <td>no</td>\n",
       "      <td>white</td>\n",
       "      <td>nothispanicorlatino</td>\n",
       "      <td>279.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>279,0</td>\n",
       "      <td>TCGA.QH.A6CZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>365</td>\n",
       "      <td>TCGA.P5.A735</td>\n",
       "      <td>38.0</td>\n",
       "      <td>astrocytoma</td>\n",
       "      <td>female</td>\n",
       "      <td>no</td>\n",
       "      <td>white</td>\n",
       "      <td>nothispanicorlatino</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>292,0</td>\n",
       "      <td>TCGA.P5.A735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>355 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0_x  Unnamed: 0.1  years_to_birth  histological_type  gender  \\\n",
       "0             474  TCGA.TQ.A7RJ            25.0   oligoastrocytoma  female   \n",
       "1             227  TCGA.HT.7468            30.0  oligodendroglioma    male   \n",
       "2              52  TCGA.DB.A75P            25.0        astrocytoma  female   \n",
       "3             320  TCGA.HT.A619            51.0  oligodendroglioma  female   \n",
       "4             221  TCGA.FG.A711            33.0  oligodendroglioma  female   \n",
       "..            ...           ...             ...                ...     ...   \n",
       "350            98  TCGA.DU.7007            33.0        astrocytoma    male   \n",
       "351           476  TCGA.TQ.A7RM            41.0   oligoastrocytoma  female   \n",
       "352           322  TCGA.HT.A61C            66.0  oligodendroglioma    male   \n",
       "353           382  TCGA.QH.A6CZ            38.0   oligoastrocytoma    male   \n",
       "354           365  TCGA.P5.A735            38.0        astrocytoma  female   \n",
       "\n",
       "    radiation_therapy   race            ethnicity  overall_survival  status  \\\n",
       "0                 yes  white     hispanicorlatino            1229.0     0.0   \n",
       "1                  no  white  nothispanicorlatino             203.0     0.0   \n",
       "2                  no    NaN                  NaN             492.0     0.0   \n",
       "3                  no  asian  nothispanicorlatino             651.0     0.0   \n",
       "4                  no  white  nothispanicorlatino            1481.0     1.0   \n",
       "..                ...    ...                  ...               ...     ...   \n",
       "350               NaN  white  nothispanicorlatino            1915.0     1.0   \n",
       "351               yes  white     hispanicorlatino            1116.0     0.0   \n",
       "352               yes  white  nothispanicorlatino             537.0     1.0   \n",
       "353                no  white  nothispanicorlatino             279.0     0.0   \n",
       "354                no  white  nothispanicorlatino             292.0     0.0   \n",
       "\n",
       "    overallsurvival  Unnamed: 0_y  \n",
       "0            1229,0  TCGA.TQ.A7RJ  \n",
       "1             203,0  TCGA.HT.7468  \n",
       "2             492,0  TCGA.DB.A75P  \n",
       "3             651,0  TCGA.HT.A619  \n",
       "4            1481,1  TCGA.FG.A711  \n",
       "..              ...           ...  \n",
       "350          1915,1  TCGA.DU.7007  \n",
       "351          1116,0  TCGA.TQ.A7RM  \n",
       "352           537,1  TCGA.HT.A61C  \n",
       "353           279,0  TCGA.QH.A6CZ  \n",
       "354           292,0  TCGA.P5.A735  \n",
       "\n",
       "[355 rows x 12 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clinical73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5a60e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clinical73=train_clinical73.drop(\"Unnamed: 0_x\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31f6fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clinical73=test_clinical.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clinical73.to_csv(\"/Users/vivit/CC/test_clinical_B73.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cab9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset1=trainset.drop(\"Unnamed: 0\",axis=1)\n",
    "testset1=testset.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ee98cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset2=trainset1.drop(\"Unnamed: 0.1\",axis=1)\n",
    "testset2=testset1.drop(\"Unnamed: 0.1\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6f403a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2BP1</th>\n",
       "      <th>A2LD1</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>A4GNT</th>\n",
       "      <th>AAA1</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>...</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "      <th>psiTPTE22</th>\n",
       "      <th>tAKR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1944</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0375</td>\n",
       "      <td>7.1808</td>\n",
       "      <td>9.0290</td>\n",
       "      <td>14.1314</td>\n",
       "      <td>7.3288</td>\n",
       "      <td>1.0252</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.4955</td>\n",
       "      <td>...</td>\n",
       "      <td>6.7406</td>\n",
       "      <td>8.8721</td>\n",
       "      <td>10.2618</td>\n",
       "      <td>0.4277</td>\n",
       "      <td>10.7750</td>\n",
       "      <td>11.9259</td>\n",
       "      <td>10.7387</td>\n",
       "      <td>9.9894</td>\n",
       "      <td>4.3204</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.2375</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.5295</td>\n",
       "      <td>5.7117</td>\n",
       "      <td>7.4545</td>\n",
       "      <td>14.2341</td>\n",
       "      <td>5.5011</td>\n",
       "      <td>0.4287</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.1342</td>\n",
       "      <td>...</td>\n",
       "      <td>6.8608</td>\n",
       "      <td>8.8308</td>\n",
       "      <td>10.1078</td>\n",
       "      <td>0.4287</td>\n",
       "      <td>10.6322</td>\n",
       "      <td>11.1134</td>\n",
       "      <td>10.4766</td>\n",
       "      <td>9.4674</td>\n",
       "      <td>10.7374</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9535</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.5235</td>\n",
       "      <td>3.9052</td>\n",
       "      <td>7.4920</td>\n",
       "      <td>13.5168</td>\n",
       "      <td>5.1915</td>\n",
       "      <td>1.2043</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.2793</td>\n",
       "      <td>...</td>\n",
       "      <td>6.4921</td>\n",
       "      <td>8.9427</td>\n",
       "      <td>10.3324</td>\n",
       "      <td>2.0911</td>\n",
       "      <td>10.7823</td>\n",
       "      <td>11.0584</td>\n",
       "      <td>10.9054</td>\n",
       "      <td>9.4311</td>\n",
       "      <td>8.0716</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6844</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.7335</td>\n",
       "      <td>4.2491</td>\n",
       "      <td>7.3253</td>\n",
       "      <td>13.4115</td>\n",
       "      <td>6.8579</td>\n",
       "      <td>0.5892</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0637</td>\n",
       "      <td>...</td>\n",
       "      <td>5.4385</td>\n",
       "      <td>8.3434</td>\n",
       "      <td>9.9855</td>\n",
       "      <td>0.5892</td>\n",
       "      <td>10.2598</td>\n",
       "      <td>10.7259</td>\n",
       "      <td>9.5133</td>\n",
       "      <td>8.8390</td>\n",
       "      <td>4.3694</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.5662</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.3424</td>\n",
       "      <td>4.9114</td>\n",
       "      <td>7.3717</td>\n",
       "      <td>14.0085</td>\n",
       "      <td>6.2716</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.5095</td>\n",
       "      <td>...</td>\n",
       "      <td>5.7292</td>\n",
       "      <td>8.6205</td>\n",
       "      <td>10.3663</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.6882</td>\n",
       "      <td>10.7732</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>9.7250</td>\n",
       "      <td>9.7346</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>7.7583</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.6741</td>\n",
       "      <td>5.3648</td>\n",
       "      <td>5.5743</td>\n",
       "      <td>15.3476</td>\n",
       "      <td>7.9331</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.9082</td>\n",
       "      <td>...</td>\n",
       "      <td>5.2734</td>\n",
       "      <td>8.2489</td>\n",
       "      <td>10.0079</td>\n",
       "      <td>0.6763</td>\n",
       "      <td>9.8439</td>\n",
       "      <td>11.4981</td>\n",
       "      <td>10.3045</td>\n",
       "      <td>8.8761</td>\n",
       "      <td>9.2236</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>6.3097</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.4914</td>\n",
       "      <td>4.8743</td>\n",
       "      <td>8.3211</td>\n",
       "      <td>13.1848</td>\n",
       "      <td>5.7910</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.8725</td>\n",
       "      <td>...</td>\n",
       "      <td>5.7772</td>\n",
       "      <td>8.4595</td>\n",
       "      <td>10.3039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.5437</td>\n",
       "      <td>10.5493</td>\n",
       "      <td>10.6046</td>\n",
       "      <td>9.3784</td>\n",
       "      <td>3.0686</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>6.4383</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.4604</td>\n",
       "      <td>4.9876</td>\n",
       "      <td>5.3276</td>\n",
       "      <td>14.1384</td>\n",
       "      <td>5.5559</td>\n",
       "      <td>1.4442</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.4867</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9364</td>\n",
       "      <td>8.9773</td>\n",
       "      <td>10.3973</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>10.4965</td>\n",
       "      <td>10.6219</td>\n",
       "      <td>10.7201</td>\n",
       "      <td>9.7107</td>\n",
       "      <td>12.8674</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>6.9881</td>\n",
       "      <td>0.4944</td>\n",
       "      <td>5.5221</td>\n",
       "      <td>4.5847</td>\n",
       "      <td>5.1757</td>\n",
       "      <td>13.4168</td>\n",
       "      <td>6.5510</td>\n",
       "      <td>1.1545</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.8779</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2746</td>\n",
       "      <td>9.0549</td>\n",
       "      <td>10.6047</td>\n",
       "      <td>1.9488</td>\n",
       "      <td>10.8586</td>\n",
       "      <td>10.9134</td>\n",
       "      <td>10.9709</td>\n",
       "      <td>9.8315</td>\n",
       "      <td>11.5579</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>7.0297</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.3438</td>\n",
       "      <td>3.7495</td>\n",
       "      <td>7.4474</td>\n",
       "      <td>12.4701</td>\n",
       "      <td>5.0503</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4627</td>\n",
       "      <td>9.7109</td>\n",
       "      <td>...</td>\n",
       "      <td>5.8221</td>\n",
       "      <td>8.5892</td>\n",
       "      <td>9.5094</td>\n",
       "      <td>2.1384</td>\n",
       "      <td>10.6923</td>\n",
       "      <td>10.9978</td>\n",
       "      <td>10.4872</td>\n",
       "      <td>9.2934</td>\n",
       "      <td>10.2409</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>355 rows × 20118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       A1BG    A1CF   A2BP1   A2LD1   A2ML1      A2M  A4GALT   A4GNT    AAA1  \\\n",
       "0    6.1944  0.0000  9.0375  7.1808  9.0290  14.1314  7.3288  1.0252  0.0000   \n",
       "1    6.2375  0.0000  8.5295  5.7117  7.4545  14.2341  5.5011  0.4287  0.0000   \n",
       "2    4.9535  0.0000  5.5235  3.9052  7.4920  13.5168  5.1915  1.2043  0.0000   \n",
       "3    4.6844  0.0000  6.7335  4.2491  7.3253  13.4115  6.8579  0.5892  0.0000   \n",
       "4    5.5662  0.0000  8.3424  4.9114  7.3717  14.0085  6.2716  0.0000  0.0000   \n",
       "..      ...     ...     ...     ...     ...      ...     ...     ...     ...   \n",
       "350  7.7583  0.0000  2.6741  5.3648  5.5743  15.3476  7.9331  0.0000  0.0000   \n",
       "351  6.3097  0.0000  7.4914  4.8743  8.3211  13.1848  5.7910  0.6115  0.0000   \n",
       "352  6.4383  0.0000  5.4604  4.9876  5.3276  14.1384  5.5559  1.4442  0.0000   \n",
       "353  6.9881  0.4944  5.5221  4.5847  5.1757  13.4168  6.5510  1.1545  0.0000   \n",
       "354  7.0297  0.0000  9.3438  3.7495  7.4474  12.4701  5.0503  0.0000  0.4627   \n",
       "\n",
       "        AAAS  ...    ZXDA    ZXDB     ZXDC  ZYG11A   ZYG11B      ZYX    ZZEF1  \\\n",
       "0     8.4955  ...  6.7406  8.8721  10.2618  0.4277  10.7750  11.9259  10.7387   \n",
       "1     9.1342  ...  6.8608  8.8308  10.1078  0.4287  10.6322  11.1134  10.4766   \n",
       "2    10.2793  ...  6.4921  8.9427  10.3324  2.0911  10.7823  11.0584  10.9054   \n",
       "3     9.0637  ...  5.4385  8.3434   9.9855  0.5892  10.2598  10.7259   9.5133   \n",
       "4     9.5095  ...  5.7292  8.6205  10.3663  0.0000  10.6882  10.7732  10.9516   \n",
       "..       ...  ...     ...     ...      ...     ...      ...      ...      ...   \n",
       "350   9.9082  ...  5.2734  8.2489  10.0079  0.6763   9.8439  11.4981  10.3045   \n",
       "351   9.8725  ...  5.7772  8.4595  10.3039  0.0000  10.5437  10.5493  10.6046   \n",
       "352   9.4867  ...  5.9364  8.9773  10.3973  0.8958  10.4965  10.6219  10.7201   \n",
       "353   9.8779  ...  6.2746  9.0549  10.6047  1.9488  10.8586  10.9134  10.9709   \n",
       "354   9.7109  ...  5.8221  8.5892   9.5094  2.1384  10.6923  10.9978  10.4872   \n",
       "\n",
       "       ZZZ3  psiTPTE22  tAKR  \n",
       "0    9.9894     4.3204   0.0  \n",
       "1    9.4674    10.7374   0.0  \n",
       "2    9.4311     8.0716   0.0  \n",
       "3    8.8390     4.3694   0.0  \n",
       "4    9.7250     9.7346   0.0  \n",
       "..      ...        ...   ...  \n",
       "350  8.8761     9.2236   0.0  \n",
       "351  9.3784     3.0686   0.0  \n",
       "352  9.7107    12.8674   0.0  \n",
       "353  9.8315    11.5579   0.0  \n",
       "354  9.2934    10.2409   0.0  \n",
       "\n",
       "[355 rows x 20118 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94e96356",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset3=trainset2.to_numpy()\n",
    "testset3=testset2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3adfeef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize inputs\n",
    "MMScaler=MinMaxScaler()\n",
    "train_scaler=MMScaler.fit_transform(trainset3)\n",
    "test_scaler=MMScaler.fit_transform(testset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5cfce4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch = torch.FloatTensor(train_scaler)\n",
    "train_loader = torch.utils.data.DataLoader(train_torch, batch_size = 256, shuffle=True)\n",
    "test_torch = torch.FloatTensor(test_scaler)\n",
    "test_loader = torch.utils.data.DataLoader(test_torch, batch_size = 256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9966630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "EPOCH=10\n",
    "lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79f965d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([355, 20118])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe126f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([153, 20118])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2fbf7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder512(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder512, self).__init__()\n",
    "        # encoder\n",
    "        self.encoder=nn.Sequential(\n",
    "            nn.Linear(in_features=20118, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=512)  \n",
    "            \n",
    "        )\n",
    "        self.decoder=nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=20118)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        encoded=self.encoder(x)\n",
    "        decoded=self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    \n",
    "model512 = Autoencoder512()\n",
    "if torch.cuda.is_available():\n",
    "    model512.cuda()\n",
    "    \n",
    "model512.train(mode=True)\n",
    "# define optimizer\n",
    "optimizer512 = torch.optim.Adam(model512.parameters(), lr=lr)\n",
    "criterion512=nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d88133b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/10], loss:0.0211\n",
      "epoch [2/10], loss:0.0194\n",
      "epoch [3/10], loss:0.0200\n",
      "epoch [4/10], loss:0.0191\n",
      "epoch [5/10], loss:0.0192\n",
      "epoch [6/10], loss:0.0192\n",
      "epoch [7/10], loss:0.0193\n",
      "epoch [8/10], loss:0.0184\n",
      "epoch [9/10], loss:0.0186\n",
      "epoch [10/10], loss:0.0171\n"
     ]
    }
   ],
   "source": [
    "# train the datasets\n",
    "for epoch in range(EPOCH):\n",
    "    for batch_features in train_torch:\n",
    "        output = model512(batch_features)\n",
    "        loss = criterion512(output, batch_features)\n",
    "        optimizer512.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer512.step()\n",
    "\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1,EPOCH,loss.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "70d2a105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0703, -0.0912,  0.0236,  ..., -0.0621, -0.0466, -0.0146],\n",
       "        [ 0.0801, -0.0862,  0.0135,  ..., -0.0530, -0.0388,  0.0027],\n",
       "        [ 0.0835, -0.0799,  0.0278,  ..., -0.0454, -0.0617, -0.0005],\n",
       "        ...,\n",
       "        [ 0.0986, -0.0965,  0.0207,  ..., -0.0262, -0.0482,  0.0035],\n",
       "        [ 0.1003, -0.0985,  0.0169,  ..., -0.0343, -0.0339, -0.0065],\n",
       "        [ 0.0771, -0.1024,  0.0116,  ..., -0.0325, -0.0392,  0.0004]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature=model512.encoder(train_torch)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74a62322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07027124 -0.09120511  0.02359719 ... -0.06210298 -0.04662137\n",
      "  -0.01456599]\n",
      " [ 0.08010261 -0.08618077  0.01351747 ... -0.05304248 -0.03882463\n",
      "   0.00272085]\n",
      " [ 0.08351094 -0.07993254  0.02780074 ... -0.04544431 -0.06166463\n",
      "  -0.00045349]\n",
      " ...\n",
      " [ 0.09855156 -0.09648471  0.02071491 ... -0.02619267 -0.04820897\n",
      "   0.00347204]\n",
      " [ 0.10033759 -0.09850293  0.01686999 ... -0.03427501 -0.03388095\n",
      "  -0.00646848]\n",
      " [ 0.07713401 -0.10243759  0.01159581 ... -0.03247996 -0.03916603\n",
      "   0.00042939]]\n"
     ]
    }
   ],
   "source": [
    "a = feature.detach().numpy()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67067c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(355, 512)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39c89501",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"/Users/vivit/Capstone/RNAseq AE results for Glioma Cancer 73.csv\",a,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5daed968",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model128, '/Users/vivit/Capstone/Glioma/AE model for RNAseq 73.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bdc7e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder256(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder256, self).__init__()\n",
    "        # encoder\n",
    "        self.encoder=nn.Sequential(\n",
    "            nn.Linear(in_features=20118, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "        )\n",
    "        self.decoder=nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=20118),\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        encoded=self.encoder(x)\n",
    "        decoded=self.decoder(encoded)\n",
    "        return encoded,decoded\n",
    "    \n",
    "    \n",
    "model256 = Autoencoder256()\n",
    "if torch.cuda.is_available():\n",
    "    model256.cuda()\n",
    "    \n",
    "model256.train(mode=True)\n",
    "# define optimizer\n",
    "optimizer256 = torch.optim.Adam(model256.parameters(), lr=lr)\n",
    "criterion256=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91abe0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/10], loss:0.0208\n",
      "epoch [2/10], loss:0.0189\n",
      "epoch [3/10], loss:0.0188\n",
      "epoch [4/10], loss:0.0189\n",
      "epoch [5/10], loss:0.0182\n",
      "epoch [6/10], loss:0.0168\n",
      "epoch [7/10], loss:0.0157\n",
      "epoch [8/10], loss:0.0155\n",
      "epoch [9/10], loss:0.0154\n",
      "epoch [10/10], loss:0.0158\n"
     ]
    }
   ],
   "source": [
    "# train the datasets\n",
    "for epoch in range(EPOCH):\n",
    "    for batch_features in train_torch:\n",
    "        _,output = model256(batch_features)\n",
    "        loss = criterion256(output, batch_features)    \n",
    "        optimizer256.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer256.step()\n",
    "\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1,EPOCH,loss.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c920bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder128, self).__init__()\n",
    "        # encoder\n",
    "        self.encoder=nn.Sequential(\n",
    "            nn.Linear(in_features=20118, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128)\n",
    "        )\n",
    "        self.decoder=nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=20118)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        encoded=self.encoder(x)\n",
    "        decoded=self.decoder(encoded)\n",
    "        return encoded,decoded\n",
    "    \n",
    "    \n",
    "model128 = Autoencoder128()\n",
    "if torch.cuda.is_available():\n",
    "    model128.cuda()\n",
    "    \n",
    "model128.train(mode=True)\n",
    "# define optimizer\n",
    "optimizer128 = torch.optim.Adam(model128.parameters(), lr=lr)\n",
    "criterion128=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68da72d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/10], loss:0.0202\n",
      "epoch [2/10], loss:0.0189\n",
      "epoch [3/10], loss:0.0178\n",
      "epoch [4/10], loss:0.0183\n",
      "epoch [5/10], loss:0.0181\n",
      "epoch [6/10], loss:0.0179\n",
      "epoch [7/10], loss:0.0167\n",
      "epoch [8/10], loss:0.0156\n",
      "epoch [9/10], loss:0.0150\n",
      "epoch [10/10], loss:0.0145\n"
     ]
    }
   ],
   "source": [
    "# train the datasets\n",
    "for epoch in range(EPOCH):\n",
    "    for batch_features in train_torch:\n",
    "        _,output = model128(batch_features)\n",
    "        loss = criterion128(output, batch_features)    \n",
    "        optimizer128.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer128.step()\n",
    "\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1,EPOCH,loss.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "caaa8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder64(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder64, self).__init__()\n",
    "        # encoder\n",
    "        self.encoder=nn.Sequential(\n",
    "            nn.Linear(in_features=20118, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "        )\n",
    "        self.decoder=nn.Sequential(\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=20118),\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        encoded=self.encoder(x)\n",
    "        decoded=self.decoder(encoded)\n",
    "        return encoded,decoded\n",
    "    \n",
    "    \n",
    "model64 = Autoencoder64()\n",
    "if torch.cuda.is_available():\n",
    "    model64.cuda()\n",
    "    \n",
    "model64.train(mode=True)\n",
    "# define optimizer\n",
    "optimizer64 = torch.optim.Adam(model64.parameters(), lr=lr)\n",
    "criterion64=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1af7747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/10], loss:0.0193\n",
      "epoch [2/10], loss:0.0190\n",
      "epoch [3/10], loss:0.0185\n",
      "epoch [4/10], loss:0.0176\n",
      "epoch [5/10], loss:0.0175\n",
      "epoch [6/10], loss:0.0175\n",
      "epoch [7/10], loss:0.0176\n",
      "epoch [8/10], loss:0.0176\n",
      "epoch [9/10], loss:0.0176\n",
      "epoch [10/10], loss:0.0172\n"
     ]
    }
   ],
   "source": [
    "# train the datasets\n",
    "for epoch in range(EPOCH):\n",
    "    for batch_features in train_torch:\n",
    "        _,output = model64(batch_features)\n",
    "        loss = criterion64(output, batch_features)    \n",
    "        optimizer64.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer64.step()\n",
    "\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1,EPOCH,loss.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15918905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=123, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "KF=KFold(n_splits=10,shuffle=True,random_state=123)\n",
    "print(KF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e29fd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,device,dataloader,loss_fun,optimizer):\n",
    "    train_loss=0.0\n",
    "    model.train()\n",
    "    for x in dataloader:\n",
    "        x=x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fun(output,x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss=train_loss/len(dataloader)\n",
    "    return train_loss\n",
    "def valid_epoch(model,device,dataloader,loss_fun):\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    for x in dataloader:\n",
    "        x=x.to(device)\n",
    "        output = model(x)\n",
    "        loss=loss_func(output,x)\n",
    "        valid_loss+=loss.item()\n",
    "    valid_loss+=valid_loss/len(dataloader)\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1f7b832",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch:1/20 AVG Training Loss:0.246 AVG Test Loss:0.664 \n",
      "Epoch:2/20 AVG Training Loss:0.328 AVG Test Loss:0.428 \n",
      "Epoch:3/20 AVG Training Loss:0.211 AVG Test Loss:0.226 \n",
      "Epoch:4/20 AVG Training Loss:0.108 AVG Test Loss:2.162 \n",
      "Epoch:5/20 AVG Training Loss:1.046 AVG Test Loss:0.151 \n",
      "Epoch:6/20 AVG Training Loss:0.069 AVG Test Loss:0.331 \n",
      "Epoch:7/20 AVG Training Loss:0.162 AVG Test Loss:0.400 \n",
      "Epoch:8/20 AVG Training Loss:0.196 AVG Test Loss:0.411 \n",
      "Epoch:9/20 AVG Training Loss:0.202 AVG Test Loss:0.395 \n",
      "Epoch:10/20 AVG Training Loss:0.194 AVG Test Loss:0.354 \n",
      "Epoch:11/20 AVG Training Loss:0.174 AVG Test Loss:0.279 \n",
      "Epoch:12/20 AVG Training Loss:0.136 AVG Test Loss:0.178 \n",
      "Epoch:13/20 AVG Training Loss:0.085 AVG Test Loss:0.176 \n",
      "Epoch:14/20 AVG Training Loss:0.083 AVG Test Loss:0.150 \n",
      "Epoch:15/20 AVG Training Loss:0.069 AVG Test Loss:0.082 \n",
      "Epoch:16/20 AVG Training Loss:0.035 AVG Test Loss:0.092 \n",
      "Epoch:17/20 AVG Training Loss:0.040 AVG Test Loss:0.096 \n",
      "Epoch:18/20 AVG Training Loss:0.041 AVG Test Loss:0.089 \n",
      "Epoch:19/20 AVG Training Loss:0.036 AVG Test Loss:0.111 \n",
      "Epoch:20/20 AVG Training Loss:0.044 AVG Test Loss:0.097 \n",
      "Fold 2\n",
      "Epoch:1/20 AVG Training Loss:0.246 AVG Test Loss:0.827 \n",
      "Epoch:2/20 AVG Training Loss:0.412 AVG Test Loss:0.434 \n",
      "Epoch:3/20 AVG Training Loss:0.215 AVG Test Loss:0.295 \n",
      "Epoch:4/20 AVG Training Loss:0.144 AVG Test Loss:0.623 \n",
      "Epoch:5/20 AVG Training Loss:0.303 AVG Test Loss:0.162 \n",
      "Epoch:6/20 AVG Training Loss:0.075 AVG Test Loss:0.272 \n",
      "Epoch:7/20 AVG Training Loss:0.132 AVG Test Loss:0.287 \n",
      "Epoch:8/20 AVG Training Loss:0.140 AVG Test Loss:0.241 \n",
      "Epoch:9/20 AVG Training Loss:0.116 AVG Test Loss:0.161 \n",
      "Epoch:10/20 AVG Training Loss:0.073 AVG Test Loss:0.196 \n",
      "Epoch:11/20 AVG Training Loss:0.087 AVG Test Loss:0.154 \n",
      "Epoch:12/20 AVG Training Loss:0.065 AVG Test Loss:0.101 \n",
      "Epoch:13/20 AVG Training Loss:0.040 AVG Test Loss:0.110 \n",
      "Epoch:14/20 AVG Training Loss:0.045 AVG Test Loss:0.101 \n",
      "Epoch:15/20 AVG Training Loss:0.041 AVG Test Loss:0.088 \n",
      "Epoch:16/20 AVG Training Loss:0.032 AVG Test Loss:0.116 \n",
      "Epoch:17/20 AVG Training Loss:0.044 AVG Test Loss:0.099 \n",
      "Epoch:18/20 AVG Training Loss:0.036 AVG Test Loss:0.088 \n",
      "Epoch:19/20 AVG Training Loss:0.032 AVG Test Loss:0.094 \n",
      "Epoch:20/20 AVG Training Loss:0.036 AVG Test Loss:0.089 \n",
      "Fold 3\n",
      "Epoch:1/20 AVG Training Loss:0.246 AVG Test Loss:1.029 \n",
      "Epoch:2/20 AVG Training Loss:0.515 AVG Test Loss:0.443 \n",
      "Epoch:3/20 AVG Training Loss:0.217 AVG Test Loss:0.311 \n",
      "Epoch:4/20 AVG Training Loss:0.150 AVG Test Loss:0.666 \n",
      "Epoch:5/20 AVG Training Loss:0.323 AVG Test Loss:0.168 \n",
      "Epoch:6/20 AVG Training Loss:0.075 AVG Test Loss:0.252 \n",
      "Epoch:7/20 AVG Training Loss:0.120 AVG Test Loss:0.252 \n",
      "Epoch:8/20 AVG Training Loss:0.120 AVG Test Loss:0.194 \n",
      "Epoch:9/20 AVG Training Loss:0.090 AVG Test Loss:0.161 \n",
      "Epoch:10/20 AVG Training Loss:0.072 AVG Test Loss:0.206 \n",
      "Epoch:11/20 AVG Training Loss:0.093 AVG Test Loss:0.120 \n",
      "Epoch:12/20 AVG Training Loss:0.049 AVG Test Loss:0.125 \n",
      "Epoch:13/20 AVG Training Loss:0.053 AVG Test Loss:0.124 \n",
      "Epoch:14/20 AVG Training Loss:0.052 AVG Test Loss:0.095 \n",
      "Epoch:15/20 AVG Training Loss:0.036 AVG Test Loss:0.105 \n",
      "Epoch:16/20 AVG Training Loss:0.038 AVG Test Loss:0.121 \n",
      "Epoch:17/20 AVG Training Loss:0.045 AVG Test Loss:0.090 \n",
      "Epoch:18/20 AVG Training Loss:0.031 AVG Test Loss:0.099 \n",
      "Epoch:19/20 AVG Training Loss:0.037 AVG Test Loss:0.101 \n",
      "Epoch:20/20 AVG Training Loss:0.038 AVG Test Loss:0.086 \n",
      "Fold 4\n",
      "Epoch:1/20 AVG Training Loss:0.245 AVG Test Loss:0.940 \n",
      "Epoch:2/20 AVG Training Loss:0.452 AVG Test Loss:0.445 \n",
      "Epoch:3/20 AVG Training Loss:0.211 AVG Test Loss:0.259 \n",
      "Epoch:4/20 AVG Training Loss:0.118 AVG Test Loss:1.648 \n",
      "Epoch:5/20 AVG Training Loss:0.777 AVG Test Loss:0.170 \n",
      "Epoch:6/20 AVG Training Loss:0.071 AVG Test Loss:0.344 \n",
      "Epoch:7/20 AVG Training Loss:0.160 AVG Test Loss:0.405 \n",
      "Epoch:8/20 AVG Training Loss:0.190 AVG Test Loss:0.408 \n",
      "Epoch:9/20 AVG Training Loss:0.192 AVG Test Loss:0.382 \n",
      "Epoch:10/20 AVG Training Loss:0.179 AVG Test Loss:0.324 \n",
      "Epoch:11/20 AVG Training Loss:0.151 AVG Test Loss:0.236 \n",
      "Epoch:12/20 AVG Training Loss:0.106 AVG Test Loss:0.176 \n",
      "Epoch:13/20 AVG Training Loss:0.074 AVG Test Loss:0.237 \n",
      "Epoch:14/20 AVG Training Loss:0.101 AVG Test Loss:0.120 \n",
      "Epoch:15/20 AVG Training Loss:0.045 AVG Test Loss:0.110 \n",
      "Epoch:16/20 AVG Training Loss:0.041 AVG Test Loss:0.123 \n",
      "Epoch:17/20 AVG Training Loss:0.047 AVG Test Loss:0.107 \n",
      "Epoch:18/20 AVG Training Loss:0.038 AVG Test Loss:0.115 \n",
      "Epoch:19/20 AVG Training Loss:0.039 AVG Test Loss:0.135 \n",
      "Epoch:20/20 AVG Training Loss:0.046 AVG Test Loss:0.108 \n",
      "Fold 5\n",
      "Epoch:1/20 AVG Training Loss:0.246 AVG Test Loss:0.942 \n",
      "Epoch:2/20 AVG Training Loss:0.460 AVG Test Loss:0.459 \n",
      "Epoch:3/20 AVG Training Loss:0.222 AVG Test Loss:0.367 \n",
      "Epoch:4/20 AVG Training Loss:0.175 AVG Test Loss:0.257 \n",
      "Epoch:5/20 AVG Training Loss:0.116 AVG Test Loss:0.120 \n",
      "Epoch:6/20 AVG Training Loss:0.050 AVG Test Loss:0.101 \n",
      "Epoch:7/20 AVG Training Loss:0.040 AVG Test Loss:0.136 \n",
      "Epoch:8/20 AVG Training Loss:0.054 AVG Test Loss:0.113 \n",
      "Epoch:9/20 AVG Training Loss:0.044 AVG Test Loss:0.119 \n",
      "Epoch:10/20 AVG Training Loss:0.045 AVG Test Loss:0.107 \n",
      "Epoch:11/20 AVG Training Loss:0.035 AVG Test Loss:0.136 \n",
      "Epoch:12/20 AVG Training Loss:0.046 AVG Test Loss:0.101 \n",
      "Epoch:13/20 AVG Training Loss:0.033 AVG Test Loss:0.109 \n",
      "Epoch:14/20 AVG Training Loss:0.039 AVG Test Loss:0.091 \n",
      "Epoch:15/20 AVG Training Loss:0.030 AVG Test Loss:0.106 \n",
      "Epoch:16/20 AVG Training Loss:0.036 AVG Test Loss:0.090 \n",
      "Epoch:17/20 AVG Training Loss:0.030 AVG Test Loss:0.089 \n",
      "Epoch:18/20 AVG Training Loss:0.031 AVG Test Loss:0.090 \n",
      "Epoch:19/20 AVG Training Loss:0.032 AVG Test Loss:0.083 \n",
      "Epoch:20/20 AVG Training Loss:0.027 AVG Test Loss:0.096 \n",
      "Fold 6\n",
      "Epoch:1/20 AVG Training Loss:0.246 AVG Test Loss:1.058 \n",
      "Epoch:2/20 AVG Training Loss:0.515 AVG Test Loss:0.448 \n",
      "Epoch:3/20 AVG Training Loss:0.217 AVG Test Loss:0.310 \n",
      "Epoch:4/20 AVG Training Loss:0.149 AVG Test Loss:0.574 \n",
      "Epoch:5/20 AVG Training Loss:0.271 AVG Test Loss:0.160 \n",
      "Epoch:6/20 AVG Training Loss:0.073 AVG Test Loss:0.254 \n",
      "Epoch:7/20 AVG Training Loss:0.120 AVG Test Loss:0.256 \n",
      "Epoch:8/20 AVG Training Loss:0.121 AVG Test Loss:0.198 \n",
      "Epoch:9/20 AVG Training Loss:0.092 AVG Test Loss:0.148 \n",
      "Epoch:10/20 AVG Training Loss:0.064 AVG Test Loss:0.225 \n",
      "Epoch:11/20 AVG Training Loss:0.097 AVG Test Loss:0.115 \n",
      "Epoch:12/20 AVG Training Loss:0.045 AVG Test Loss:0.112 \n",
      "Epoch:13/20 AVG Training Loss:0.045 AVG Test Loss:0.119 \n",
      "Epoch:14/20 AVG Training Loss:0.049 AVG Test Loss:0.098 \n",
      "Epoch:15/20 AVG Training Loss:0.038 AVG Test Loss:0.102 \n",
      "Epoch:16/20 AVG Training Loss:0.038 AVG Test Loss:0.119 \n",
      "Epoch:17/20 AVG Training Loss:0.045 AVG Test Loss:0.087 \n",
      "Epoch:18/20 AVG Training Loss:0.031 AVG Test Loss:0.088 \n",
      "Epoch:19/20 AVG Training Loss:0.033 AVG Test Loss:0.094 \n",
      "Epoch:20/20 AVG Training Loss:0.036 AVG Test Loss:0.086 \n",
      "Fold 7\n",
      "Epoch:1/20 AVG Training Loss:0.246 AVG Test Loss:0.974 \n",
      "Epoch:2/20 AVG Training Loss:0.500 AVG Test Loss:0.429 \n",
      "Epoch:3/20 AVG Training Loss:0.217 AVG Test Loss:0.310 \n",
      "Epoch:4/20 AVG Training Loss:0.154 AVG Test Loss:0.488 \n",
      "Epoch:5/20 AVG Training Loss:0.243 AVG Test Loss:0.152 \n",
      "Epoch:6/20 AVG Training Loss:0.070 AVG Test Loss:0.220 \n",
      "Epoch:7/20 AVG Training Loss:0.106 AVG Test Loss:0.208 \n",
      "Epoch:8/20 AVG Training Loss:0.100 AVG Test Loss:0.154 \n",
      "Epoch:9/20 AVG Training Loss:0.071 AVG Test Loss:0.174 \n",
      "Epoch:10/20 AVG Training Loss:0.079 AVG Test Loss:0.159 \n",
      "Epoch:11/20 AVG Training Loss:0.070 AVG Test Loss:0.111 \n",
      "Epoch:12/20 AVG Training Loss:0.046 AVG Test Loss:0.117 \n",
      "Epoch:13/20 AVG Training Loss:0.049 AVG Test Loss:0.107 \n",
      "Epoch:14/20 AVG Training Loss:0.043 AVG Test Loss:0.087 \n",
      "Epoch:15/20 AVG Training Loss:0.031 AVG Test Loss:0.112 \n",
      "Epoch:16/20 AVG Training Loss:0.042 AVG Test Loss:0.101 \n",
      "Epoch:17/20 AVG Training Loss:0.037 AVG Test Loss:0.087 \n",
      "Epoch:18/20 AVG Training Loss:0.031 AVG Test Loss:0.095 \n",
      "Epoch:19/20 AVG Training Loss:0.036 AVG Test Loss:0.089 \n",
      "Epoch:20/20 AVG Training Loss:0.033 AVG Test Loss:0.081 \n",
      "Fold 8\n",
      "Epoch:1/20 AVG Training Loss:0.245 AVG Test Loss:0.989 \n",
      "Epoch:2/20 AVG Training Loss:0.487 AVG Test Loss:0.443 \n",
      "Epoch:3/20 AVG Training Loss:0.215 AVG Test Loss:0.301 \n",
      "Epoch:4/20 AVG Training Loss:0.144 AVG Test Loss:0.870 \n",
      "Epoch:5/20 AVG Training Loss:0.416 AVG Test Loss:0.173 \n",
      "Epoch:6/20 AVG Training Loss:0.078 AVG Test Loss:0.288 \n",
      "Epoch:7/20 AVG Training Loss:0.138 AVG Test Loss:0.318 \n",
      "Epoch:8/20 AVG Training Loss:0.153 AVG Test Loss:0.291 \n",
      "Epoch:9/20 AVG Training Loss:0.139 AVG Test Loss:0.226 \n",
      "Epoch:10/20 AVG Training Loss:0.107 AVG Test Loss:0.188 \n",
      "Epoch:11/20 AVG Training Loss:0.086 AVG Test Loss:0.241 \n",
      "Epoch:12/20 AVG Training Loss:0.109 AVG Test Loss:0.144 \n",
      "Epoch:13/20 AVG Training Loss:0.061 AVG Test Loss:0.134 \n",
      "Epoch:14/20 AVG Training Loss:0.057 AVG Test Loss:0.134 \n",
      "Epoch:15/20 AVG Training Loss:0.057 AVG Test Loss:0.104 \n",
      "Epoch:16/20 AVG Training Loss:0.041 AVG Test Loss:0.101 \n",
      "Epoch:17/20 AVG Training Loss:0.036 AVG Test Loss:0.133 \n",
      "Epoch:18/20 AVG Training Loss:0.050 AVG Test Loss:0.095 \n",
      "Epoch:19/20 AVG Training Loss:0.032 AVG Test Loss:0.103 \n",
      "Epoch:20/20 AVG Training Loss:0.038 AVG Test Loss:0.107 \n",
      "Fold 9\n",
      "Epoch:1/20 AVG Training Loss:0.246 AVG Test Loss:2.524 \n",
      "Epoch:2/20 AVG Training Loss:1.258 AVG Test Loss:0.460 \n",
      "Epoch:3/20 AVG Training Loss:0.227 AVG Test Loss:0.419 \n",
      "Epoch:4/20 AVG Training Loss:0.206 AVG Test Loss:0.210 \n",
      "Epoch:5/20 AVG Training Loss:0.101 AVG Test Loss:0.857 \n",
      "Epoch:6/20 AVG Training Loss:0.417 AVG Test Loss:0.107 \n",
      "Epoch:7/20 AVG Training Loss:0.047 AVG Test Loss:0.241 \n",
      "Epoch:8/20 AVG Training Loss:0.116 AVG Test Loss:0.316 \n",
      "Epoch:9/20 AVG Training Loss:0.154 AVG Test Loss:0.332 \n",
      "Epoch:10/20 AVG Training Loss:0.162 AVG Test Loss:0.310 \n",
      "Epoch:11/20 AVG Training Loss:0.150 AVG Test Loss:0.251 \n",
      "Epoch:12/20 AVG Training Loss:0.119 AVG Test Loss:0.165 \n",
      "Epoch:13/20 AVG Training Loss:0.074 AVG Test Loss:0.159 \n",
      "Epoch:14/20 AVG Training Loss:0.066 AVG Test Loss:0.205 \n",
      "Epoch:15/20 AVG Training Loss:0.087 AVG Test Loss:0.110 \n",
      "Epoch:16/20 AVG Training Loss:0.042 AVG Test Loss:0.093 \n",
      "Epoch:17/20 AVG Training Loss:0.037 AVG Test Loss:0.108 \n",
      "Epoch:18/20 AVG Training Loss:0.046 AVG Test Loss:0.103 \n",
      "Epoch:19/20 AVG Training Loss:0.043 AVG Test Loss:0.086 \n",
      "Epoch:20/20 AVG Training Loss:0.034 AVG Test Loss:0.094 \n",
      "Fold 10\n",
      "Epoch:1/20 AVG Training Loss:0.246 AVG Test Loss:1.061 \n",
      "Epoch:2/20 AVG Training Loss:0.520 AVG Test Loss:0.447 \n",
      "Epoch:3/20 AVG Training Loss:0.218 AVG Test Loss:0.288 \n",
      "Epoch:4/20 AVG Training Loss:0.139 AVG Test Loss:0.963 \n",
      "Epoch:5/20 AVG Training Loss:0.463 AVG Test Loss:0.165 \n",
      "Epoch:6/20 AVG Training Loss:0.076 AVG Test Loss:0.308 \n",
      "Epoch:7/20 AVG Training Loss:0.148 AVG Test Loss:0.357 \n",
      "Epoch:8/20 AVG Training Loss:0.173 AVG Test Loss:0.353 \n",
      "Epoch:9/20 AVG Training Loss:0.170 AVG Test Loss:0.313 \n",
      "Epoch:10/20 AVG Training Loss:0.151 AVG Test Loss:0.241 \n",
      "Epoch:11/20 AVG Training Loss:0.114 AVG Test Loss:0.180 \n",
      "Epoch:12/20 AVG Training Loss:0.081 AVG Test Loss:0.254 \n",
      "Epoch:13/20 AVG Training Loss:0.114 AVG Test Loss:0.148 \n",
      "Epoch:14/20 AVG Training Loss:0.063 AVG Test Loss:0.115 \n",
      "Epoch:15/20 AVG Training Loss:0.048 AVG Test Loss:0.123 \n",
      "Epoch:16/20 AVG Training Loss:0.053 AVG Test Loss:0.104 \n",
      "Epoch:17/20 AVG Training Loss:0.043 AVG Test Loss:0.091 \n",
      "Epoch:18/20 AVG Training Loss:0.035 AVG Test Loss:0.130 \n",
      "Epoch:19/20 AVG Training Loss:0.052 AVG Test Loss:0.102 \n",
      "Epoch:20/20 AVG Training Loss:0.038 AVG Test Loss:0.094 \n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset \n",
    "K_valid_loss=[]\n",
    "for fold, (train_idx,val_idx) in enumerate(KF.split(np.arange(len(test_torch)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = torch.utils.data.DataLoader(train_torch,batch_size=512,sampler=train_sampler)\n",
    "    test_loader = torch.utils.data.DataLoader(test_torch,batch_size=512,sampler=test_sampler)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Autoencoder512()\n",
    "    model.to(device)\n",
    "    loss_func=nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    history = {'train_loss': [], 'test_loss': []}\n",
    "    for epoch in range(EPOCH):\n",
    "        train_loss=train_epoch(model,device,train_loader,criterion,optimizer)\n",
    "        test_loss=valid_epoch(model,device,test_loader,criterion)\n",
    "        print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} \".format(epoch + 1,EPOCH,train_loss, test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
